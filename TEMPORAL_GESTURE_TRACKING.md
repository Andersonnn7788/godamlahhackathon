# ğŸ¯ Temporal Gesture Tracking System

## ğŸ› The Problem

**Original Issue**: The system was only detecting **static hand positions** (single frames), but sign language is **dynamic** and requires **movement tracking** over time.

### Why Static Detection Fails:
- âŒ **Single frame analysis**: Only looks at one moment in time
- âŒ **No movement context**: Can't distinguish between similar static poses
- âŒ **Unstable predictions**: Flickers between different labels
- âŒ **Missing dynamic gestures**: Can't detect signs that require movement (e.g., waving, circular motions)

## âœ… The Solution: Gesture Sequence Tracker

A **temporal analysis system** that tracks hand movements over multiple frames to recognize dynamic sign language gestures.

### Key Features:

#### 1. **Frame Buffer (10 frames)**
```typescript
private frameBuffer: DetectionFrame[] = [];
private readonly maxBufferSize = 10; // Last 10 frames (~5 seconds)
```
- Stores recent detections with timestamps
- Tracks hand position changes over time
- Automatically removes old frames

#### 2. **Movement Pattern Detection**
```typescript
type MovementPattern = 'static' | 'moving' | 'complex';
```

| Pattern | Description | Example Signs |
|---------|-------------|---------------|
| **Static** | Hand stays in same position | "SAYA" (me), "NAMA" (name) |
| **Moving** | Simple directional movement | "DATANG" (come), "PERGI" (go) |
| **Complex** | Multiple direction changes | "TOLONG" (help), circular motions |

#### 3. **Gesture Stability Check**
```typescript
isStableGesture(): boolean {
  // Returns true if same label detected for 3+ consecutive frames
  return recentFrames.every(frame => frame.label === firstLabel);
}
```

**Benefits**:
- âœ… Reduces false positives
- âœ… Only sends stable gestures to AI
- âœ… Filters out transition movements

#### 4. **Confidence Boosting**
```typescript
getGestureConfidence(): number {
  let confidence = sequence.averageConfidence;
  
  // Boost for stable gestures
  if (this.isStableGesture()) {
    confidence *= 1.2; // +20% boost
  }
  
  // Reduce for short sequences
  if (sequence.frames.length < 3) {
    confidence *= 0.8; // -20% penalty
  }
  
  return Math.min(confidence, 1.0);
}
```

## ğŸ”„ How It Works

### Frame-by-Frame Process:

```
Frame 1: "SAYA" (67%) â†’ Added to buffer [1 frame]
  â†“
Frame 2: "SAYA" (72%) â†’ Added to buffer [2 frames]
  â†“
Frame 3: "SAYA" (89%) â†’ Added to buffer [3 frames] âœ… STABLE!
  â†“
  â†’ Send to AI for interpretation
  â†’ Display: "ğŸŸ¢ Static - SAYA (89%)"
```

### Movement Detection:

```
Frame 1: Hand at (100, 200)
  â†“
Frame 2: Hand at (150, 200) â†’ Moving RIGHT
  â†“
Frame 3: Hand at (200, 200) â†’ Moving RIGHT
  â†“
  â†’ Pattern: "ğŸŸ¡ Moving - DATANG (85%)"
```

### Complex Gesture:

```
Frame 1: Hand at (100, 200)
  â†“
Frame 2: Hand at (150, 180) â†’ Moving RIGHT + UP
  â†“
Frame 3: Hand at (180, 220) â†’ Moving RIGHT + DOWN
  â†“
Frame 4: Hand at (150, 250) â†’ Moving LEFT + DOWN
  â†“
  â†’ Pattern: "ğŸ”µ Complex - TOLONG (78%)"
```

## ğŸ“Š UI Indicators

### Movement Pattern Badges:

```tsx
<Badge variant={movementPattern === 'static' ? 'default' : 'warning'}>
  <Activity className="h-3 w-3" />
  {movementPattern === 'static' && 'ğŸŸ¢ Static'}
  {movementPattern === 'moving' && 'ğŸŸ¡ Moving'}
  {movementPattern === 'complex' && 'ğŸ”µ Complex'}
</Badge>
```

### Gesture Confidence:
```tsx
<Badge variant="success">
  {Math.round(gestureConfidence * 100)}% confidence
</Badge>
```

### Frame Count:
```tsx
<span className="text-gray-500">
  ({sequenceTracker.current.getFrameCount()} frames)
</span>
```

## ğŸ¬ Example Console Output

### Static Gesture:
```
ğŸ“¸ Frame captured, sending to detection...
ğŸ”„ Testing working models for best detection...
ğŸ“¤ Testing MYSL...
âœ… MYSL: SAYA (87%)
ğŸ“¤ Testing Mothana...
âœ… Mothana: SAYA (89%)
ğŸ“¤ Testing BIM v10...
âœ… BIM v10: SAYA (85%)

ğŸ“Š Gesture Analysis:
   Label: SAYA
   Pattern: static
   Frames: 3
   Confidence: 87.0%

ğŸ† BEST: Mothana detected "SAYA" (89.1%)
ğŸ“Š Total detections: 3 models found signs
âœ… Stable gesture detected - sending to AI for interpretation
```

### Moving Gesture:
```
ğŸ“Š Gesture Analysis:
   Label: DATANG
   Pattern: moving
   Frames: 5
   Confidence: 82.5%

â³ Building gesture sequence... (5 frames)
```

### Building Sequence:
```
ğŸ“Š Gesture Analysis:
   Label: TOLONG
   Pattern: complex
   Frames: 2
   Confidence: 65.0%

â³ Building gesture sequence... (2 frames)
```

## âš™ï¸ Configuration

### Tunable Parameters:

```typescript
class GestureSequenceTracker {
  private readonly maxBufferSize = 10;           // Max frames to keep
  private readonly sequenceTimeout = 5000;       // 5 seconds
  private readonly movementThreshold = 50;       // 50 pixels
  private readonly stabilityThreshold = 3;       // 3 frames = stable
}
```

### Capture Timing:

```typescript
captureInterval={2000}  // 2000ms = 0.5 FPS
```

**Why 0.5 FPS?**
- âœ… Fast enough to track movement
- âœ… Slow enough to avoid overwhelming Roboflow API
- âœ… Balances accuracy vs. speed
- âœ… 10 frames = 20 seconds of gesture history

## ğŸš€ Performance Improvements

### Before (Static Detection):
- âŒ Flickering labels
- âŒ False positives
- âŒ No movement tracking
- âŒ Unstable AI interpretation

### After (Temporal Tracking):
- âœ… Stable label detection
- âœ… Movement pattern recognition
- âœ… Confidence boosting for stable gestures
- âœ… Only sends stable gestures to AI
- âœ… Visual feedback for movement type

## ğŸ“ˆ Benefits

### 1. **Better Accuracy**
- Averages confidence across multiple frames
- Filters out noise and transitions
- Boosts confidence for stable gestures

### 2. **Movement Recognition**
- Detects static vs. moving vs. complex patterns
- Tracks hand position changes
- Recognizes directional movements

### 3. **Reduced API Calls**
- Only sends stable gestures to AI
- Waits for 3+ consecutive frames
- Saves costs and improves speed

### 4. **Better UX**
- Visual feedback for movement type
- Frame count indicator
- Clear gesture sequence button
- Real-time confidence updates

## ğŸ¯ Next Steps for Further Improvement

### 1. **Video-Based Models**
Consider using video-based sign language models:
- **MediaPipe Holistic**: Tracks hands + body + face
- **Temporal Segment Networks (TSN)**: Video classification
- **I3D (Inflated 3D ConvNets)**: 3D video understanding

### 2. **Custom Training**
Train models on Malaysian Sign Language (BIM) video sequences:
- Record video clips of full signs
- Label with start/end timestamps
- Train on temporal sequences

### 3. **LSTM/Transformer Models**
Use sequence models for better temporal understanding:
- **LSTM**: Long Short-Term Memory for sequences
- **Transformers**: Attention-based sequence modeling
- **Temporal Convolutional Networks (TCN)**: 1D convolutions over time

### 4. **Gesture Grammar**
Implement sign language grammar rules:
- **Word order**: BIM has different grammar than English
- **Facial expressions**: Important for meaning
- **Non-manual markers**: Head nods, eyebrow raises

## ğŸ“š References

- **MediaPipe Hands**: https://google.github.io/mediapipe/solutions/hands.html
- **Sign Language Recognition**: https://paperswithcode.com/task/sign-language-recognition
- **Temporal Action Detection**: https://paperswithcode.com/task/temporal-action-localization
- **Malaysian Sign Language (BIM)**: https://www.jkm.gov.my/

## ğŸ”§ Troubleshooting

### Issue: Labels still flickering
**Solution**: Increase `stabilityThreshold` to 4 or 5 frames

### Issue: Gestures not being sent to AI
**Solution**: Check that `isStableGesture()` returns true (3+ consecutive frames with same label)

### Issue: Movement pattern always shows "static"
**Solution**: Reduce `movementThreshold` to 30 pixels or increase capture rate

### Issue: Too slow to detect
**Solution**: Reduce `captureInterval` to 1500ms or use fewer models

---

**Status**: âœ… Implemented and tested
**Version**: 1.0.0
**Last Updated**: December 9, 2025

